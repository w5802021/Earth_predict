{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General information\n",
    "\n",
    "In this kernel I'll try various technics for models interpretability and feature selection. Also I'll compare various models.\n",
    "\n",
    "I use the features from my dataset: https://www.kaggle.com/artgor/lanl-features\n",
    "\n",
    "This dataset was created using this kernel: https://www.kaggle.com/artgor/even-more-features/\n",
    "\n",
    "*work in progress*\n",
    "\n",
    "![](https://torontoseoulcialite.com/wp-content/uploads/2016/02/zimbiocom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "pd.options.display.precision = 15\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn import svm\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import datetime\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import neighbors\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import linear_model\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.ensemble import ExtraTreesRegressor, AdaBoostRegressor\n",
    "\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import hann\n",
    "from scipy.signal import convolve\n",
    "from scipy import stats\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import librosa, librosa.display\n",
    "import builtins\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import eli5\n",
    "import shap\n",
    "from sklearn.feature_selection import GenericUnivariateSelect, SelectPercentile, SelectKBest, f_classif, mutual_info_classif, RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing and setting up altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from  altair.vega import v3\n",
    "from IPython.display import HTML\n",
    "import json\n",
    "# Preparing altair. I use code from this great kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\n",
    "\n",
    "vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v3.SCHEMA_VERSION\n",
    "vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n",
    "vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n",
    "vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n",
    "noext = \"?noext\"\n",
    "\n",
    "paths = {\n",
    "    'vega': vega_url + noext,\n",
    "    'vega-lib': vega_lib_url + noext,\n",
    "    'vega-lite': vega_lite_url + noext,\n",
    "    'vega-embed': vega_embed_url + noext\n",
    "}\n",
    "\n",
    "workaround = \"\"\"\n",
    "requirejs.config({{\n",
    "    baseUrl: 'https://cdn.jsdelivr.net/npm/',\n",
    "    paths: {}\n",
    "}});\n",
    "\"\"\"\n",
    "\n",
    "#------------------------------------------------ Defs for future rendering\n",
    "def add_autoincrement(render_func):\n",
    "    # Keep track of unique <div/> IDs\n",
    "    cache = {}\n",
    "    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n",
    "        if autoincrement:\n",
    "            if id in cache:\n",
    "                counter = 1 + cache[id]\n",
    "                cache[id] = counter\n",
    "            else:\n",
    "                cache[id] = 0\n",
    "            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n",
    "        else:\n",
    "            if id not in cache:\n",
    "                cache[id] = 0\n",
    "            actual_id = id\n",
    "        return render_func(chart, id=actual_id)\n",
    "    # Cache will stay outside and \n",
    "    return wrapped\n",
    "            \n",
    "@add_autoincrement\n",
    "def render(chart, id=\"vega-chart\"):\n",
    "    chart_str = \"\"\"\n",
    "    <div id=\"{id}\"></div><script>\n",
    "    require([\"vega-embed\"], function(vg_embed) {{\n",
    "        const spec = {chart};     \n",
    "        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n",
    "        console.log(\"anything?\");\n",
    "    }});\n",
    "    console.log(\"really...anything?\");\n",
    "    </script>\n",
    "    \"\"\"\n",
    "    return HTML(\n",
    "        chart_str.format(\n",
    "            id=id,\n",
    "            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n",
    "        )\n",
    "    )\n",
    "\n",
    "HTML(\"\".join((\n",
    "    \"<script>\",\n",
    "    workaround.format(json.dumps(paths)),\n",
    "    \"</script>\",\n",
    ")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('../input/lanl-features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 'train_features.csv' - train features generated on original data\n",
    "* 'train_features_denoised.csv' - train features generated on denoised data\n",
    "* 'test_features.csv' - test features generated on original data\n",
    "* 'test_features_denoised.csv' - test features generated on denoised data\n",
    "* 'submission_1.csv' - one of my local submissions\n",
    "* 'y.csv' - train target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "Let's load features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../input/lanl-features/train_features.csv')\n",
    "test_features = pd.read_csv('../input/lanl-features/test_features.csv')\n",
    "train_features_denoised = pd.read_csv('../input/lanl-features/train_features_denoised.csv')\n",
    "test_features_denoised = pd.read_csv('../input/lanl-features/test_features_denoised.csv')\n",
    "train_features_denoised.columns = [f'{i}_denoised' for i in train_features_denoised.columns]\n",
    "test_features_denoised.columns = [f'{i}_denoised' for i in test_features_denoised.columns]\n",
    "y = pd.read_csv('../input/lanl-features/y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([train_features, train_features_denoised], axis=1).drop(['seg_id_denoised', 'target_denoised'], axis=1)\n",
    "X_test = pd.concat([test_features, test_features_denoised], axis=1).drop(['seg_id_denoised', 'target_denoised'], axis=1)\n",
    "X = X[:-1]\n",
    "y = y[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have almost 2000 features here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function\n",
    "\n",
    "Training function is in the hidden cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def train_model(X, X_test, y, params, folds, model_type='lgb', plot_feature_importance=False, model=None):\n",
    "\n",
    "    oof = np.zeros(len(X))\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print('Fold', fold_n, 'started at', time.ctime())\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[train_index], X[valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\n",
    "                    verbose=10000, early_stopping_rounds=200)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=500, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = mean_absolute_error(y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. MAE: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric='MAE', **params, loss_function='MAE')\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb' and plot_feature_importance:\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = X.columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= n_fold\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        if plot_feature_importance:\n",
    "            feature_importance[\"importance\"] /= n_fold\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "        \n",
    "            return oof, prediction, feature_importance\n",
    "        return oof, prediction, scores\n",
    "    \n",
    "    else:\n",
    "        return oof, prediction, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 128,\n",
    "          'min_child_samples': 79,\n",
    "          'objective': 'gamma',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"subsample_freq\": 5,\n",
    "          \"subsample\": 0.9,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1302650970728192,\n",
    "          'reg_lambda': 0.3603427518866501,\n",
    "          'colsample_bytree': 0.1\n",
    "         }\n",
    "oof_lgb, prediction_lgb, feature_importance = train_model(X, X_test, y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/LANL-Earthquake-Prediction/sample_submission.csv', index_col='seg_id')\n",
    "submission['time_to_failure'] = prediction_lgb\n",
    "print(submission.head())\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv('../input/lanl-features/submission_1.csv')\n",
    "sub1.to_csv('submission_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN features\n",
    "Here I normalize the data and create features using NearestNeighbors. The idea is to find samples which are similar and use it to generate features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X), columns=X.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n = 10\n",
    "neigh = NearestNeighbors(n, n_jobs=-1)\n",
    "neigh.fit(X_train_scaled)\n",
    "\n",
    "dists, _ = neigh.kneighbors(X_train_scaled, n_neighbors=n)\n",
    "mean_dist = dists.mean(axis=1)\n",
    "max_dist = dists.max(axis=1)\n",
    "min_dist = dists.min(axis=1)\n",
    "\n",
    "X_train_scaled['mean_dist'] = mean_dist\n",
    "X_train_scaled['max_dist'] = max_dist\n",
    "X_train_scaled['min_dist'] = min_dist\n",
    "\n",
    "test_dists, _ = neigh.kneighbors(X_test_scaled, n_neighbors=n)\n",
    "\n",
    "test_mean_dist = test_dists.mean(axis=1)\n",
    "test_max_dist = test_dists.max(axis=1)\n",
    "test_min_dist = test_dists.min(axis=1)\n",
    "\n",
    "X_test_scaled['mean_dist'] = test_mean_dist\n",
    "X_test_scaled['max_dist'] = test_max_dist\n",
    "X_test_scaled['min_dist'] = test_min_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'num_leaves': 32,\n",
    "          'min_data_in_leaf': 79,\n",
    "          'objective': 'gamma',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"bagging_freq\": 5,\n",
    "          \"bagging_fraction\": 0.8126672064208567,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1302650970728192,\n",
    "          'reg_lambda': 0.3603427518866501,\n",
    "          'feature_fraction': 0.1\n",
    "         }\n",
    "oof_lgb, prediction_lgb, feature_importance = train_model(X_train_scaled, X_test_scaled, y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['time_to_failure'] = prediction_lgb\n",
    "submission.to_csv('submission_nn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELI5 and permutation importance\n",
    "ELI5 is a package with provides explanations for ML models. It can do this not only for linear models, but also for tree based like Random Forest or lightgbm.\n",
    "\n",
    "**Important notice**: running eli5 on all features takes a lot of time, so I run the cell below in `version 14` and printed the top-50 features. In the following versions I'll use these 50 columns and use eli5 to find top-40 of them so that it takes less time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_columns = ['iqr1_denoised', 'percentile_5_denoised', 'abs_percentile_90_denoised', 'percentile_95_denoised', 'ave_roll_std_10', 'num_peaks_10', 'percentile_roll_std_20',\n",
    "               'ratio_unique_values_denoised', 'fftr_percentile_roll_std_75_denoised', 'num_crossing_0_denoised', 'percentile_95', 'ffti_percentile_roll_std_75_denoised',\n",
    "               'min_roll_std_10000', 'percentile_roll_std_1', 'percentile_roll_std_10', 'fftr_percentile_roll_std_70_denoised', 'ave_roll_std_50', 'ffti_percentile_roll_std_70_denoised',\n",
    "               'exp_Moving_std_300_mean_denoised', 'ffti_percentile_roll_std_30_denoised', 'mean_change_rate', 'percentile_roll_std_5', 'range_-1000_0', 'mad',\n",
    "               'fftr_range_1000_2000_denoised', 'percentile_10_denoised', 'ffti_percentile_roll_std_80', 'percentile_roll_std_25', 'fftr_percentile_10_denoised',\n",
    "               'ffti_range_-2000_-1000_denoised', 'autocorrelation_5', 'min_roll_std_100', 'fftr_percentile_roll_std_80', 'min_roll_std_500', 'min_roll_std_50', 'min_roll_std_1000',\n",
    "               'ffti_percentile_20_denoised', 'iqr1', 'classic_sta_lta5_mean_denoised', 'classic_sta_lta6_mean_denoised', 'percentile_roll_std_10_denoised',\n",
    "               'fftr_percentile_70_denoised', 'ffti_c3_50_denoised', 'ffti_percentile_roll_std_75', 'abs_percentile_90', 'range_0_1000', 'spkt_welch_density_50_denoised',\n",
    "               'ffti_percentile_roll_std_40_denoised', 'ffti_range_-4000_-3000', 'mean_change_rate_last_50000']\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X[top_columns], y, test_size=0.1)\n",
    "model = lgb.LGBMRegressor(**params, n_estimators = 50000, n_jobs = -1, verbose=-1)\n",
    "model.fit(X_train, y_train, \n",
    "        eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='mae',\n",
    "        verbose=10000, early_stopping_rounds=200)\n",
    "\n",
    "perm = eli5.sklearn.PermutationImportance(model, random_state=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(perm, top=50, feature_names=top_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = [i for i in eli5.formatters.as_dataframe.explain_weights_df(model).feature if 'BIAS' not in i][:40]\n",
    "oof_lgb, prediction_lgb, feature_importance = train_model(X[top_features], X_test[top_features], y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['time_to_failure'] = prediction_lgb\n",
    "submission.to_csv('submission_eli5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "Here I try various approaches to feature selection.\n",
    "\n",
    "**Important notice**: running feature selection on all features takes a lot of time, so I'll run some of feature selection methods and print the result, which I'll use in the following versions of the kernel, so that I can explore more approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "params = {'num_leaves': 32,\n",
    "          'min_child_samples': 79,\n",
    "          'objective': 'gamma',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"subsample_freq\": 5,\n",
    "          \"subsample\": 0.9,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1302650970728192,\n",
    "          'reg_lambda': 0.3603427518866501,\n",
    "          'colsample_bytree': 1.0\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectPercentile\n",
    "\n",
    "**Important notice**:  I run the cell below in `version 14` and printed the scores_dict. In the following versions I'll use `scores_dict` and plot the results instead of running feature selection each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scores_dict = {'f_classif': [], 'mutual_info_classif': [], 'n_features': []}\n",
    "# for i in range(5, 105, 5):\n",
    "#     print(i)\n",
    "#     s1 = SelectPercentile(f_classif, percentile=i)\n",
    "#     X_train1 = s1.fit_transform(X, y.values.astype(int))\n",
    "#     X_test1 = s1.transform(X_test)\n",
    "#     oof, prediction, scores = train_model(X_train1, X_test1, y.values.reshape(-1, ), params=params, folds=folds, model_type='lgb', plot_feature_importance=False)\n",
    "#     scores_dict['f_classif'].append(np.mean(scores))\n",
    "    \n",
    "#     s2 = SelectPercentile(mutual_info_classif, percentile=i)\n",
    "#     X_train1 = s2.fit_transform(X, y.values.astype(int))\n",
    "#     X_test1 = s2.transform(X_test)\n",
    "#     oof, prediction, scores = train_model(X_train1, X_test1, y.values.reshape(-1, ), params=params, folds=folds, model_type='lgb', plot_feature_importance=False)\n",
    "#     scores_dict['mutual_info_classif'].append(np.mean(scores))\n",
    "    \n",
    "#     scores_dict['n_features'].append(X_train1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {'f_classif': [2.0746468465171377, 2.0753843541953687, 2.062191535440333, 2.0654327826583034, 2.0643551320704936, 2.0617560048382675,\n",
    "                             2.061565197738015, 2.0598878198917494, 2.0654865223333143, 2.0632788555735777, 2.058002635080971, 2.051075689018734,\n",
    "                             2.0472543961304583, 2.052401474353084, 2.055924154798443, 2.0561794619762352, 2.0549680611994963, 2.057123777802326,\n",
    "                             2.0591868861136904, 2.0577745274024553],\n",
    "               'mutual_info_classif': [2.0866763775014006, 2.0745431497064324, 2.0564324832516427, 2.060125564781158, 2.067334544167612, 2.0665943783246448,\n",
    "                                       2.063891669849029, 2.070194051004794, 2.0667490707700447, 2.0681653852378354, 2.0592743636982345, 2.061260741522344,\n",
    "                                       2.05680667824411, 2.0565047875243003, 2.058252567141659, 2.0554927194831922, 2.0562776429736873, 2.0618179277444084,\n",
    "                                       2.06364125584214, 2.0577745274024553],\n",
    "               'n_features': [98, 196, 294, 392, 490, 588, 685, 783, 881, 979, 1077, 1175, 1273, 1370, 1468, 1566, 1664, 1762, 1860, 1958]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores_dict)\n",
    "scores_df = scores_df.melt(id_vars=['n_features'], value_vars=['mutual_info_classif', 'f_classif'], var_name='metric', value_name='mae')\n",
    "max_value = scores_df['mae'].max() * 1.05\n",
    "min_value = scores_df['mae'].min() * 0.95\n",
    "render(alt.Chart(scores_df).mark_line().encode(\n",
    "    y=alt.Y('mae:Q', scale=alt.Scale(domain=(min_value, max_value))),\n",
    "    x='n_features:O',\n",
    "    color='metric:N',\n",
    "    tooltip=['metric:N', 'n:O', 'mae:Q']\n",
    ").properties(\n",
    "    title='Top N features by SelectPercentile vs CV'\n",
    ").interactive())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectKBest\n",
    "\n",
    "**Important notice**:  I run the cell below in `version 14` and printed the scores_dict. In the following versions I'll use `scores_dict` and plot the results instead of running feature selection each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scores_dict = {'f_classif': [], 'mutual_info_classif': [], 'n_features': []}\n",
    "# for i in np.arange(10, 1958, 100):\n",
    "#     print(i)\n",
    "#     s1 = SelectKBest(f_classif, k=i)\n",
    "#     X_train1 = s1.fit_transform(X, y.values.astype(int))\n",
    "#     X_test1 = s1.transform(X_test)\n",
    "#     oof, prediction, scores = train_model(X_train1, X_test1, y.values.reshape(-1, ), params=params, folds=folds, model_type='lgb', plot_feature_importance=False)\n",
    "#     scores_dict['f_classif'].append(np.mean(scores))\n",
    "    \n",
    "#     s2 = SelectKBest(mutual_info_classif, k=i)\n",
    "#     X_train1 = s2.fit_transform(X, y.values.astype(int))\n",
    "#     X_test1 = s2.transform(X_test)\n",
    "#     oof, prediction, scores = train_model(X_train1, X_test1, y.values.reshape(-1, ), params=params, folds=folds, model_type='lgb', plot_feature_importance=False)\n",
    "#     scores_dict['mutual_info_classif'].append(np.mean(scores))\n",
    "    \n",
    "#     scores_dict['n_features'].append(X_train1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {'f_classif': [2.1495892622081354, 2.0778182269587147, 2.0716153738740006, 2.06152950679902, 2.0645162758752553, 2.0627705797004032, 2.0610992303725157,\n",
    "                             2.057762113735462, 2.0618360883613627, 2.0603197111525984, 2.06081274633874, 2.0580767195278056, 2.0527646572747127, 2.0498353445032533,\n",
    "                             2.052442594925, 2.0564456881902133, 2.0582284644115365, 2.0558612960548635, 2.0580900016350094, 2.058218782401599],\n",
    "               'mutual_info_classif': [2.1235703196243687, 2.084958198672301, 2.0596822478390955, 2.053305869981444, 2.063468853227225, 2.0674399950434323, 2.0658618511287874,\n",
    "                                       2.063003703200445, 2.0653174905858664, 2.0644340327023656, 2.0748993062333523, 2.0587602096358113, 2.0601495560836076, 2.0559629138548603,\n",
    "                                       2.0553852701221134, 2.058022171415446, 2.060755947658241, 2.057916705462307, 2.056245795262636, 2.0580691870837056],\n",
    "               'n_features': [10, 110, 210, 310, 410, 510, 610, 710, 810, 910, 1010, 1110, 1210, 1310, 1410, 1510, 1610, 1710, 1810, 1910]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores_dict)\n",
    "scores_df = scores_df.melt(id_vars=['n_features'], value_vars=['mutual_info_classif', 'f_classif'], var_name='metric', value_name='mae')\n",
    "max_value = scores_df['mae'].max() * 1.05\n",
    "min_value = scores_df['mae'].min() * 0.95\n",
    "render(alt.Chart(scores_df).mark_line().encode(\n",
    "    y=alt.Y('mae:Q', scale=alt.Scale(domain=(min_value, max_value))),\n",
    "    x='n_features:O',\n",
    "    color='metric:N',\n",
    "    tooltip=['metric:N', 'n:O', 'mae:Q']\n",
    ").properties(\n",
    "    title='Top N features by SelectKBest vs CV'\n",
    ").interactive())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping highly correlated features\n",
    "\n",
    "Due to the huge number of features there are certainly some highly correlated features, let's try droping them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n",
    "corr_matrix = X.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.99)]\n",
    "X = X.drop(to_drop, axis=1)\n",
    "X_test = X_test.drop(to_drop, axis=1)\n",
    "oof_lgb, prediction_lgb, feature_importance = train_model(X, X_test, y, params=params, folds=folds, model_type='lgb', plot_feature_importance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['time_to_failure'] = prediction_lgb\n",
    "submission.to_csv('submission_no_corr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFE\n",
    "\n",
    "\n",
    "**Important notice**:  I run the cell below in `version 18` and printed the scores_dict. In the following versions I'll use `scores_dict` and plot the results instead of running feature selection each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scores_dict = {'rfe_score': [], 'n_features': []}\n",
    "# for i in np.arange(10, 1958, 100)[:3]:\n",
    "#     print(i)\n",
    "#     s1 = RFE(model, i, step=100)\n",
    "#     X_train1 = s1.fit_transform(X, y.values.astype(int))\n",
    "#     X_test1 = s1.transform(X_test)\n",
    "#     oof, prediction, scores = train_model(X_train1, X_test1, y.values.reshape(-1, ), params=params, folds=folds, model_type='lgb', plot_feature_importance=False)\n",
    "#     scores_dict['rfe_score'].append(np.mean(scores))\n",
    "    \n",
    "#     scores_dict['n_features'].append(X_train1.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {'rfe_score': [2.103586938061856, 2.052535910798748, 2.053228199447811], 'n_features': [10, 110, 210]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores_dict)\n",
    "scores_df = scores_df.melt(id_vars=['n_features'], value_vars=['rfe_score'], var_name='metric', value_name='mae')\n",
    "max_value = scores_df['mae'].max() * 1.05\n",
    "min_value = scores_df['mae'].min() * 0.95\n",
    "render(alt.Chart(scores_df).mark_line().encode(\n",
    "    y=alt.Y('mae:Q', scale=alt.Scale(domain=(min_value, max_value))),\n",
    "    x='n_features:O',\n",
    "    color='metric:N',\n",
    "    tooltip=['metric:N', 'n:O', 'mae:Q']\n",
    ").properties(\n",
    "    title='Top N features by RFE vs CV'\n",
    ").interactive())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison\n",
    "\n",
    "In this section I'll try variuos sklearn models and compair their score. Running GridSearchCV each time is too long, so I'll run it once for each model and use optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rfr = RandomForestRegressor()\n",
    "\n",
    "# parameter_grid = {'n_estimators': [50, 60],\n",
    "#                   'max_depth': [5, 10]\n",
    "#                  }\n",
    "\n",
    "# grid_search = GridSearchCV(rfr, param_grid=parameter_grid, cv=folds, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "# grid_search.fit(X, y)\n",
    "# print('Best score: {}'.format(grid_search.best_score_))\n",
    "# print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "# rfr = RandomForestRegressor(**grid_search.best_params_)\n",
    "rfr = RandomForestRegressor(n_estimators=50, max_depth=5)\n",
    "oof_rfr, prediction_rfr, scores_rfr = train_model(X, X_test, y, params=params, folds=folds, model_type='sklearn', model=rfr)\n",
    "print(scores_rfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "linreg = linear_model.LinearRegression(normalize=True, copy_X=True, n_jobs=-1)\n",
    "\n",
    "# parameter_grid = {'n_estimators': [50, 60],\n",
    "#                   'max_depth': [5, 10]\n",
    "#                  }\n",
    "\n",
    "# grid_search = GridSearchCV(rfr, param_grid=parameter_grid, cv=folds, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "# grid_search.fit(X, y)\n",
    "# print('Best score: {}'.format(grid_search.best_score_))\n",
    "# print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "# rfr = RandomForestRegressor(**grid_search.best_params_)\n",
    "# rfr = RandomForestRegressor(n_estimators=50, max_depth=5)\n",
    "oof_linreg, prediction_linreg, scores_linreg = train_model(X, X_test, y, params=params, folds=folds, model_type='sklearn', model=linreg)\n",
    "# print(scores_linreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ridge = linear_model.Ridge(normalize=True)\n",
    "\n",
    "parameter_grid = {'alpha': [0.1, 1.0, 10.0]}\n",
    "\n",
    "grid_search = GridSearchCV(ridge, param_grid=parameter_grid, cv=folds, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "ridge = linear_model.Ridge(**grid_search.best_params_, normalize=True)\n",
    "oof_ridge, prediction_ridge, scores_ridge = train_model(X, X_test, y, params=params, folds=folds, model_type='sklearn', model=ridge)\n",
    "# print(scores_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn = neighbors.KNeighborsRegressor()\n",
    "\n",
    "parameter_grid = {'n_neighbors': [50, 100]}\n",
    "\n",
    "grid_search = GridSearchCV(knn, param_grid=parameter_grid, cv=folds, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "knn = neighbors.KNeighborsRegressor(**grid_search.best_params_)\n",
    "oof_knn, prediction_knn, scores_knn = train_model(X, X_test, y, params=params, folds=folds, model_type='sklearn', model=knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "lasso = linear_model.Lasso(normalize=True)\n",
    "\n",
    "parameter_grid = {'alpha': [0.1, 1.0, 10.0]}\n",
    "\n",
    "grid_search = GridSearchCV(lasso, param_grid=parameter_grid, cv=folds, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "lasso = linear_model.Lasso(**grid_search.best_params_, normalize=True)\n",
    "oof_lasso, prediction_lasso, scores_lasso = train_model(X, X_test, y, params=params, folds=folds, model_type='sklearn', model=lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "etr = ExtraTreesRegressor()\n",
    "\n",
    "parameter_grid = {'n_estimators': [500, 1000],\n",
    "                  'max_depth': [5, 10, 20]\n",
    "                 }\n",
    "\n",
    "grid_search = GridSearchCV(rfr, param_grid=parameter_grid, cv=folds, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "etr = ExtraTreesRegressor(**grid_search.best_params_)\n",
    "oof_etr, prediction_etr, scores_etr = train_model(X, X_test, y, params=params, folds=folds, model_type='sklearn', model=etr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "huber = linear_model.HuberRegressor()\n",
    "\n",
    "parameter_grid = {'epsilon ': [0.1, 1.0, 10.0]}\n",
    "\n",
    "grid_search = GridSearchCV(huber, param_grid=parameter_grid, cv=folds, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "huber = linear_model.huber(**grid_search.best_params_)\n",
    "oof_huber, prediction_huber, scores_huber = train_model(X, X_test, y, params=params, folds=folds, model_type='sklearn', model=huber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "adr = AdaBoostRegressor()\n",
    "\n",
    "parameter_grid = {'n_estimators': [10, 50, 100, 500],\n",
    "                 }\n",
    "\n",
    "grid_search = GridSearchCV(adr, param_grid=parameter_grid, cv=folds, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "adr = AdaBoostRegressor(**grid_search.best_params_)\n",
    "oof_adr, prediction_adr, scores_adr = train_model(X, X_test, y, params=params, folds=folds, model_type='sklearn', model=adr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8));\n",
    "scores_df = pd.DataFrame({'RandomForestRegressor': scores_rfr})\n",
    "scores_df['ExtraTreesRegressor'] = scores_etr\n",
    "scores_df['AdaBoostRegressor'] = scores_adr\n",
    "scores_df['KNN'] = scores_knn\n",
    "scores_df['LinearRegression'] = scores_linreg\n",
    "scores_df['Ridge'] = scores_ridge\n",
    "scores_df['Lasso'] = scores_lasso\n",
    "scores_df['Huber'] = scores_huber\n",
    "\n",
    "sns.boxplot(data=scores_df);\n",
    "plt.xticks(rotation=45);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
