{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import NuSVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.core import Activation, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.utils import np_utils\n",
    "import scipy.stats as scs\n",
    "import scipy.signal as sig\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float32})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp = train['acoustic_data'][:100]\n",
    "temp_smooth = temp.rolling(10).mean()\n",
    "print(len(sig.find_peaks(temp_smooth.values)[0]))\n",
    "\n",
    "plt.plot(temp)\n",
    "plt.plot(temp_smooth)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling = train.acoustic_data.rolling(window=50).quantile(0.25)\n",
    "# rolling[]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 150000\n",
    "segments = int(np.floor(train.shape[0] / rows))\n",
    "col_names = ['fft_{}'.format(i) for i in range(20)]\n",
    "col_names = ['ave', 'std'] + col_names\n",
    "\n",
    "\n",
    "X_train = pd.DataFrame(index=range(segments), dtype=np.float64,\n",
    "                       columns=col_names)\n",
    "y_train = pd.DataFrame(index=range(segments), dtype=np.float64,\n",
    "                       columns=['time_to_failure'])\n",
    "for segment in tqdm(range(segments)):\n",
    "    seg = train.iloc[segment*rows:segment*rows+rows]\n",
    "    seg_smooth = seg.rolling(10).mean()\n",
    "\n",
    "    x = seg['acoustic_data'].values\n",
    "    x_smooth = seg_smooth['acoustic_data'].values\n",
    "    y = seg['time_to_failure'].values[-1]\n",
    "    y_train.loc[segment, 'time_to_failure'] = y\n",
    "    \n",
    "    X_train.loc[segment, 'ave'] = x.mean()\n",
    "    X_train.loc[segment, 'abs_mean'] = np.absolute(x).mean()\n",
    "    X_train.loc[segment, 'std'] = x.std()\n",
    "#     X_train.loc[segment, 'max'] = x.max()\n",
    "#     X_train.loc[segment, 'min'] = x.min()\n",
    "    X_train.loc[segment, 'log_diff'] = np.log(x.ptp())\n",
    "    \n",
    "    X_train.loc[segment, 'kurt'] = scs.kurtosis(x)\n",
    "    X_train.loc[segment, 'skew'] = scs.skew(x)\n",
    "    \n",
    "#     X_train.loc[segment, 'p25'] = np.percentile(x,0.25)\n",
    "#     X_train.loc[segment, 'p50'] = np.percentile(x,0.50)\n",
    "#     X_train.loc[segment, 'p75'] = np.percentile(x,0.75)\n",
    "#     X_train.loc[segment, 'p80'] = np.percentile(x,0.80)\n",
    "#     X_train.loc[segment, 'p85'] = np.percentile(x,0.85)\n",
    "#     X_train.loc[segment, 'p90'] = np.percentile(x,0.90)\n",
    "#     X_train.loc[segment, 'p95'] = np.percentile(x,0.95)\n",
    "#     X_train.loc[segment, 'pinter'] = X_train.loc[segment, 'p75'] - X_train.loc[segment, 'p25']\n",
    "    \n",
    "    for i in range(0,10,2):\n",
    "        temp_range = x[int(len(x)*i/10):int(len(x)*(i+2)/10)]\n",
    "        temp_range_smooth = x_smooth[int(len(x)*i/10):int(len(x)*(i+2)/10)]\n",
    "        X_train.loc[segment, f'ave{10*i}'] = temp_range.mean()\n",
    "        X_train.loc[segment, f'std{10*i}'] = temp_range.std()\n",
    "#         X_train.loc[segment, 'max{}'.format(10*i)] = x[int(len(x)*i/10):int(len(x)*(i+2)/10)].max()\n",
    "#         X_train.loc[segment, 'min{}'.format(10*i)] = x[int(len(x)*i/10):int(len(x)*(i+2)/10)].min()\n",
    "        X_train.loc[segment, f'log_diff{10*i}'] = np.log(temp_range.ptp())\n",
    "        X_train.loc[segment, f'peaks_smooth{10*i}'] = len(sig.find_peaks(temp_range_smooth)[0])\n",
    "        X_train.loc[segment, f'zeros_smooth{10*i}'] = ((temp_range_smooth[:-1] * temp_range_smooth[1:]) < 0).sum()\n",
    "        X_train.loc[segment, f'peaks_diff_smooth{10*i}'] = np.mean(np.diff(sig.find_peaks(temp_range_smooth)[0]))\n",
    "        \n",
    "    freq = np.fft.fft(x, n=50).real\n",
    "    freq_i = np.fft.fft(x, n=50).imag\n",
    "    \n",
    "    X_train.loc[segment, 'fftp80'] = np.percentile(freq,0.80)\n",
    "    X_train.loc[segment, 'fftp85'] = np.percentile(freq,0.85)\n",
    "    X_train.loc[segment, 'fftp90'] = np.percentile(freq,0.90)\n",
    "    X_train.loc[segment, 'fftp95'] = np.percentile(freq,0.95)\n",
    "    \n",
    "    X_train.loc[segment, 'fftip80'] = np.percentile(freq_i,0.80)\n",
    "    X_train.loc[segment, 'fftip85'] = np.percentile(freq_i,0.85)\n",
    "    X_train.loc[segment, 'fftip90'] = np.percentile(freq_i,0.90)\n",
    "    X_train.loc[segment, 'fftip95'] = np.percentile(freq_i,0.95)\n",
    "    \n",
    "    for idx, freq_val in enumerate(freq):\n",
    "        X_train.loc[segment, f'fft_{idx}'] = freq_val\n",
    "        \n",
    "    for idx, freq_val in enumerate(freq_i):\n",
    "        X_train.loc[segment, f'ffti_{idx}'] = freq_val\n",
    "        \n",
    "    X_train.loc[segment, 'peaks_smooth'] = len(sig.find_peaks(x_smooth)[0])\n",
    "    X_train.loc[segment, 'zeros_smooth'] = ((x_smooth[:-1] * x_smooth[1:]) < 0).sum()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = NuSVR(nu=0.5, verbose=True, kernel='rbf', degree=3, tol=0.00005, gamma='scale', shrinking=True)\n",
    "svm.fit(X_train_scaled, y_train.values.flatten())\n",
    "y_pred = svm.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(X_train_scaled, label=y_train.values.flatten())\n",
    "params = {'num_leaves': 128,\n",
    "          'min_child_samples': 79,\n",
    "          'objective': 'gamma',\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.01,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"subsample_freq\": 5,\n",
    "          \"subsample\": 0.9,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1302650970728192,\n",
    "          'reg_lambda': 0.3603427518866501,\n",
    "          'colsample_bytree': 0.1\n",
    "         }\n",
    "\n",
    "clf = lgb.train(params, d_train)\n",
    "y_pred = clf.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design a Deep Feedforward Neural Network and Its Training\n",
    "## Several major considerations: network architecture, activation function, loss function, dropout, regularization.\t\n",
    "def DefineModel_FNN():\n",
    "    ################################################################\n",
    "    ## Network Structure\n",
    "    first_layer_width = 10\n",
    "    second_layer_width = 20\n",
    "    ## Try more layers\n",
    "    third_layer_width = 0\n",
    "    forth_layer_width = 0\n",
    "    fifth_layer_width = 0\n",
    "    \n",
    "    ################################################################\n",
    "    ## Activation Function: relu, sigmoid, tanh, elu\n",
    "    activation_func = 'relu' \n",
    "    # activation_func = 'sigmoid'\n",
    "    # activation_func = 'tanh'\n",
    "    ################################################################\n",
    "    ################################################################    \n",
    "    ## Loss Function:\n",
    "    #\n",
    "    loss_function = 'mae'\n",
    "    # loss_function = 'mean_squared_error'\n",
    "    ################################################################\n",
    "    \n",
    "    ################################################################    \n",
    "    ## Dropout option\n",
    "    #\n",
    "    dropout_rate = 0.2\n",
    "    # dropout_rate = 0.5\n",
    "    # dropout_rate = 0.9\n",
    "    ################################################################ \n",
    "    \n",
    "    ################################################################    \n",
    "    ## Regularization option\n",
    "    #\n",
    "    # weight_regularizer = l1(0.01)\n",
    "#     weight_regularizer = l2(0.01)\n",
    "    weight_regularizer = None\n",
    "    ################################################################\n",
    "    ################################################################    \n",
    "    ## Learning Rate\n",
    "    learning_rate = 0.025\n",
    "    # learning_rate = 0.0001\n",
    "    # learning_rate = 0.5\n",
    "    ################################################################\n",
    "    \n",
    "    ## Initialize model.\n",
    "    model = Sequential()\n",
    "    ## First hidden layer with 'first_layer_width' neurons. \n",
    "    ## Also need to specify input dimension.\n",
    "    ## 'Dense' means fully-connected.\n",
    "    model.add( Dense(first_layer_width, input_dim=146, W_regularizer=weight_regularizer) )\n",
    "    model.add( Activation(activation_func) )\n",
    "    if dropout_rate > 0:\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    ## Second hidden layer.\n",
    "    if second_layer_width > 0:\n",
    "        model.add( Dense(second_layer_width) )\n",
    "        model.add( Activation(activation_func) )\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(dropout_rate)) \n",
    "    \n",
    "    ## Third hidden layer.\n",
    "    if third_layer_width > 0:\n",
    "        model.add( Dense(third_layer_width) )\n",
    "        model.add( Activation(activation_func) )\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(dropout_rate)) \n",
    "            \n",
    "    ## forth hidden layer.\n",
    "    if forth_layer_width > 0:\n",
    "        model.add( Dense(forth_layer_width) )\n",
    "        model.add( Activation(activation_func) )\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(dropout_rate)) \n",
    "    \n",
    "    ## fifth hidden layer.\n",
    "    if fifth_layer_width > 0:\n",
    "        model.add( Dense(fifth_layer_width) )\n",
    "        model.add( Activation(activation_func) )\n",
    "        if dropout_rate > 0:\n",
    "            model.add( Dropout(dropout_rate) )\n",
    "    \n",
    "    ## Last layer has the same dimension as the number of classes\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    ## Define optimizer. In this tutorial/codelab, we select SGD.\n",
    "    ## You can also use other methods, e.g., opt = RMSprop()\n",
    "    opt = SGD(lr=learning_rate, clipnorm=5.)\n",
    "    # opt = 'adam'\n",
    "    \n",
    "    ## Compile the model\n",
    "    model.compile(loss=loss_function, optimizer=opt, metrics=['mse'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.transform(X_train)\n",
    "# model = DefineModel_FNN()\n",
    "model = KerasRegressor(build_fn=DefineModel_FNN)\n",
    "model.fit(X_train_scaled, y_train.values.flatten(),epochs=300)\n",
    "y_pred = model.predict(X_train_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(model, random_state=1).fit(X_train_scaled, y_train.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(perm, top=200, feature_names = X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_train.values.flatten(), y_pred)\n",
    "plt.xlim(0, 20)\n",
    "plt.ylim(0, 20)\n",
    "plt.xlabel('actual', fontsize=12)\n",
    "plt.ylabel('predicted', fontsize=12)\n",
    "plt.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = mean_absolute_error(y_train.values.flatten(), y_pred)\n",
    "print(f'Score: {score:0.3f}')\n",
    "# 2.277 w/ log_diff, var\n",
    "# 2.270 w/ log_diff\n",
    "# 2.207 w/ log_diff + last 10% data::fnn TURNED IN WITH SCORE 1.612\n",
    "# 2.173 increased epochs, increased layer width, added one layer, lowered learning rate\n",
    "# 2.143 doubled epochs\n",
    "# 2.169 doubled batch size\n",
    "# 2.493 lowered learning to 0.001\n",
    "# 2.111 back to 0.025 lr, doubled epochs\n",
    "# 2.108 added layer w/ 40 nodes\n",
    "# 2.081 added 10 nodes to layer 1 TURNED IN WITH LOWER SCORE 1.631\n",
    "# 2.121 Lowered epocks \n",
    "# 1.884 Added back in ptp, var, and fft features\n",
    "# 1.594 fft increased from 20 to 50 features\n",
    "# 0.065 Large network with 10% features added, total 113 TURNED IN WITH SCORE 2.017\n",
    "# 1.903 Added droppout and reduced features to 88 TURNED IN WITH SCORE 1.747\n",
    "# Same features with svm TURNED IN WITH LOWER SCORE NOPE\n",
    "# 1.929 Added regularization, reduced layer size TURNED IN WITH SCORE 1.84\n",
    "# 1.653 Increased fft to 100 TURNED IN WITH SCORE 1.913\n",
    "# 1.444 50 fft real and 50 fft imag\n",
    "# 1.380 reduced net size to 3 layers of 50, epochs to 300, and dropout to 0.2\n",
    "# 2.341 lightgbm model using grandmaster parameters\n",
    "# 1.230 Took out regularization TURNED IN WITH SCORE 1.902\n",
    "# 1.813 Dropout to 0.3, reduced layer size to 30 30 30 TURNED IN WITH SCORE 1.951\n",
    "# 1.609 Increased epochs to 500\n",
    "# 1.614 Feature changes (more percentiles and took out variance and max/min) with weight information\n",
    "# 1.580 Added fft and ffti percentiles COMMIT\n",
    "# 1.771 Removed third layer\n",
    "# 1.363 Added smoothed data peaks wow\n",
    "# 1.485 Added zero crossings TURNED IN WITH SCORE 2.064\n",
    "# 1.925 Same as above with SVM TURNED IN WITH SCORE 1.675\n",
    "# 1.795 Two layers of size 20, 300 epochs\n",
    "# 1.661 Removed percentile features and added rolling zero crosses TURNED IN WITH SCORE 1.831\n",
    "# 1.919 Same as above with SVM \n",
    "# 1.910 Added avg distance between peaks\n",
    "# 2.161 lightgbm TURNED IN WITH SCORE 1.919\n",
    "# 1.769 Reduced network and dropout \n",
    "# 1.715 SVR with nu = 1\n",
    "# 1.712 changed gamma to scale commited\n",
    "# 2.178 nu = 0.2 committed\n",
    "# 1.909 nu = 0.5 committed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv', index_col='seg_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(columns=X_train.columns, dtype=np.float64, index=submission.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seg_id in X_test.index:\n",
    "    seg = pd.read_csv('../input/test/' + seg_id + '.csv')\n",
    "    seg_smooth = seg.rolling(10).mean()\n",
    "    \n",
    "    x = seg['acoustic_data'].values\n",
    "    x_smooth = seg_smooth['acoustic_data'].values\n",
    "    \n",
    "    \n",
    "    X_test.loc[seg_id, 'ave'] = x.mean()\n",
    "    X_test.loc[seg_id, 'abs_mean'] = np.absolute(x).mean()\n",
    "    X_test.loc[seg_id, 'std'] = x.std()\n",
    "#     X_test.loc[seg_id, 'max'] = x.max()\n",
    "#     X_test.loc[seg_id, 'min'] = x.min()\n",
    "    X_test.loc[seg_id, 'log_diff'] = np.log(x.max()-x.min())\n",
    "    \n",
    "    X_test.loc[seg_id, 'kurt'] = scs.kurtosis(x)\n",
    "    X_test.loc[seg_id, 'skew'] = scs.skew(x)\n",
    "    \n",
    "#     X_test.loc[seg_id, 'p25'] = np.percentile(x,0.25)\n",
    "#     X_test.loc[seg_id, 'p50'] = np.percentile(x,0.50)\n",
    "#     X_test.loc[seg_id, 'p75'] = np.percentile(x,0.75)\n",
    "#     X_test.loc[seg_id, 'p80'] = np.percentile(x,0.80)\n",
    "#     X_test.loc[seg_id, 'p85'] = np.percentile(x,0.85)\n",
    "#     X_test.loc[seg_id, 'p90'] = np.percentile(x,0.90)\n",
    "#     X_test.loc[seg_id, 'p95'] = np.percentile(x,0.95)\n",
    "#     X_test.loc[seg_id, 'pinter'] = X_test.loc[seg_id, 'p75'] - X_test.loc[seg_id, 'p25']\n",
    "    \n",
    "    for i in range(0,10,2):\n",
    "        temp_x = x[int(len(x)*i/10):int(len(x)*(i+2)/10)]\n",
    "        temp_x_smooth = x_smooth[int(len(x)*i/10):int(len(x)*(i+2)/10)]\n",
    "        X_test.loc[seg_id, 'ave{}'.format(10*i)] = temp_x.mean()\n",
    "        X_test.loc[seg_id, 'std{}'.format(10*i)] = temp_x.std()\n",
    "#         X_test.loc[seg_id, 'max{}'.format(10*i)] = x[int(len(x)*i/10):int(len(x)*(i+2)/10)].max()\n",
    "#         X_test.loc[seg_id, 'min{}'.format(10*i)] = x[int(len(x)*i/10):int(len(x)*(i+2)/10)].min()\n",
    "        X_test.loc[seg_id, 'log_diff{}'.format(10*i)] = np.log(temp_x.max()-temp_x.min())\n",
    "        X_test.loc[seg_id, f'peaks_smooth{10*i}'] = len(sig.find_peaks(temp_x_smooth)[0])\n",
    "        X_test.loc[seg_id, f'zeros_smooth{10*i}'] = ((temp_x_smooth[:-1] * temp_x_smooth[1:]) < 0).sum()\n",
    "        X_test.loc[seg_id, f'peaks_diff_smooth{10*i}'] = np.mean(np.diff(sig.find_peaks(temp_range_smooth)[0]))\n",
    "    \n",
    "    freq = np.fft.fft(x, n=50).real\n",
    "    freq_i = np.fft.fft(x, n=50).imag\n",
    "    \n",
    "    \n",
    "    X_test.loc[seg_id, 'fftp80'] = np.percentile(freq,0.80)\n",
    "    X_test.loc[seg_id, 'fftp85'] = np.percentile(freq,0.85)\n",
    "    X_test.loc[seg_id, 'fftp90'] = np.percentile(freq,0.90)\n",
    "    X_test.loc[seg_id, 'fftp95'] = np.percentile(freq,0.95)\n",
    "    \n",
    "    X_test.loc[seg_id, 'fftip80'] = np.percentile(freq_i,0.80)\n",
    "    X_test.loc[seg_id, 'fftip85'] = np.percentile(freq_i,0.85)\n",
    "    X_test.loc[seg_id, 'fftip90'] = np.percentile(freq_i,0.90)\n",
    "    X_test.loc[seg_id, 'fftip95'] = np.percentile(freq_i,0.95)\n",
    "    \n",
    "    for idx, freq_val in enumerate(freq):\n",
    "        X_test.loc[seg_id, 'fft_{}'.format(idx)] = freq_val\n",
    "        \n",
    "    for idx, freq_val in enumerate(freq_i):\n",
    "        X_test.loc[seg_id, 'ffti_{}'.format(idx)] = freq_val\n",
    "    \n",
    "    X_test.loc[seg_id, 'peaks_smooth'] = len(sig.find_peaks(x_smooth)[0])\n",
    "    X_test.loc[seg_id, 'zeros_smooth'] = ((x_smooth[:-1] * x_smooth[1:]) < 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(X_test_scaled.size)\n",
    "# print(X_test_scaled[:10])\n",
    "# print(model.predict(X_test_scaled))\n",
    "submission['time_to_failure'] = svm.predict(X_test_scaled)\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('submission.csv')\n",
    "sub[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
