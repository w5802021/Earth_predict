{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "import scipy as sp\n",
    "from sklearn import metrics\n",
    "from tsfresh.feature_extraction import feature_calculators\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "class FeatureGenerator(object):\n",
    "    def __init__(self, dtype, n_jobs=1, chunk_size=None):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.dtype = dtype\n",
    "        self.filename = None\n",
    "        self.n_jobs = n_jobs\n",
    "        self.test_files = []\n",
    "        if self.dtype == 'train':\n",
    "            self.filename = '../input/train.csv'\n",
    "            self.total_data = int(629145481 / self.chunk_size)\n",
    "        else:\n",
    "            submission = pd.read_csv('../input/sample_submission.csv')\n",
    "            for seg_id in submission.seg_id.values:\n",
    "                self.test_files.append((seg_id, '../input/test/' + seg_id + '.csv'))\n",
    "            self.total_data = int(len(submission))\n",
    "\n",
    "    def read_chunks(self):\n",
    "        if self.dtype == 'train':\n",
    "            iter_df = pd.read_csv(self.filename, iterator=True, chunksize=self.chunk_size,\n",
    "                                  dtype={'acoustic_data': np.float64, 'time_to_failure': np.float64})\n",
    "            for counter, df in enumerate(iter_df):\n",
    "                x = df.acoustic_data.values\n",
    "                y = df.time_to_failure.values[-1]\n",
    "                seg_id = 'train_' + str(counter)\n",
    "                del df\n",
    "                yield seg_id, x, y\n",
    "        else:\n",
    "            for seg_id, f in self.test_files:\n",
    "                df = pd.read_csv(f, dtype={'acoustic_data': np.float64})\n",
    "                x = df.acoustic_data.values[-self.chunk_size:]\n",
    "                del df\n",
    "                yield seg_id, x, -999\n",
    "\n",
    "    def features(self, x, y, seg_id):\n",
    "        feature_dict = dict()\n",
    "        feature_dict['target'] = y\n",
    "        feature_dict['seg_id'] = seg_id\n",
    "\n",
    "        # create features here\n",
    "        # numpy\n",
    "        feature_dict['mean'] = np.mean(x)\n",
    "        feature_dict['max'] = np.max(x)\n",
    "        feature_dict['min'] = np.min(x)\n",
    "        feature_dict['std'] = np.std(x)\n",
    "        feature_dict['var'] = np.var(x)\n",
    "        feature_dict['ptp'] = np.ptp(x)\n",
    "        feature_dict['percentile_10'] = np.percentile(x, 10)\n",
    "        feature_dict['percentile_20'] = np.percentile(x, 20)\n",
    "        feature_dict['percentile_30'] = np.percentile(x, 30)\n",
    "        feature_dict['percentile_40'] = np.percentile(x, 40)\n",
    "        feature_dict['percentile_50'] = np.percentile(x, 50)\n",
    "        feature_dict['percentile_60'] = np.percentile(x, 60)\n",
    "        feature_dict['percentile_70'] = np.percentile(x, 70)\n",
    "        feature_dict['percentile_80'] = np.percentile(x, 80)\n",
    "        feature_dict['percentile_90'] = np.percentile(x, 90)\n",
    "\n",
    "        # scipy\n",
    "        feature_dict['skew'] = sp.stats.skew(x)\n",
    "        feature_dict['kurtosis'] = sp.stats.kurtosis(x)\n",
    "        feature_dict['kstat_1'] = sp.stats.kstat(x, 1)\n",
    "        feature_dict['kstat_2'] = sp.stats.kstat(x, 2)\n",
    "        feature_dict['kstat_3'] = sp.stats.kstat(x, 3)\n",
    "        feature_dict['kstat_4'] = sp.stats.kstat(x, 4)\n",
    "        feature_dict['moment_1'] = sp.stats.moment(x, 1)\n",
    "        feature_dict['moment_2'] = sp.stats.moment(x, 2)\n",
    "        feature_dict['moment_3'] = sp.stats.moment(x, 3)\n",
    "        feature_dict['moment_4'] = sp.stats.moment(x, 4)\n",
    "        \n",
    "        feature_dict['abs_energy'] = feature_calculators.abs_energy(x)\n",
    "        feature_dict['abs_sum_of_changes'] = feature_calculators.absolute_sum_of_changes(x)\n",
    "        feature_dict['count_above_mean'] = feature_calculators.count_above_mean(x)\n",
    "        feature_dict['count_below_mean'] = feature_calculators.count_below_mean(x)\n",
    "        feature_dict['mean_abs_change'] = feature_calculators.mean_abs_change(x)\n",
    "        feature_dict['mean_change'] = feature_calculators.mean_change(x)\n",
    "        feature_dict['var_larger_than_std_dev'] = feature_calculators.variance_larger_than_standard_deviation(x)\n",
    "        feature_dict['range_minf_m4000'] = feature_calculators.range_count(x, -np.inf, -4000)\n",
    "        feature_dict['range_m4000_m3000'] = feature_calculators.range_count(x, -4000, -3000)\n",
    "        feature_dict['range_m3000_m2000'] = feature_calculators.range_count(x, -3000, -2000)\n",
    "        feature_dict['range_m2000_m1000'] = feature_calculators.range_count(x, -2000, -1000)\n",
    "        feature_dict['range_m1000_0'] = feature_calculators.range_count(x, -1000, 0)\n",
    "        feature_dict['range_0_p1000'] = feature_calculators.range_count(x, 0, 1000)\n",
    "        feature_dict['range_p1000_p2000'] = feature_calculators.range_count(x, 1000, 2000)\n",
    "        feature_dict['range_p2000_p3000'] = feature_calculators.range_count(x, 2000, 3000)\n",
    "        feature_dict['range_p3000_p4000'] = feature_calculators.range_count(x, 3000, 4000)\n",
    "        feature_dict['range_p4000_pinf'] = feature_calculators.range_count(x, 4000, np.inf)\n",
    "\n",
    "        feature_dict['ratio_unique_values'] = feature_calculators.ratio_value_number_to_time_series_length(x)\n",
    "        feature_dict['first_loc_min'] = feature_calculators.first_location_of_minimum(x)\n",
    "        feature_dict['first_loc_max'] = feature_calculators.first_location_of_maximum(x)\n",
    "        feature_dict['last_loc_min'] = feature_calculators.last_location_of_minimum(x)\n",
    "        feature_dict['last_loc_max'] = feature_calculators.last_location_of_maximum(x)\n",
    "        feature_dict['time_rev_asym_stat_10'] = feature_calculators.time_reversal_asymmetry_statistic(x, 10)\n",
    "        feature_dict['time_rev_asym_stat_100'] = feature_calculators.time_reversal_asymmetry_statistic(x, 100)\n",
    "        feature_dict['time_rev_asym_stat_1000'] = feature_calculators.time_reversal_asymmetry_statistic(x, 1000)\n",
    "        feature_dict['autocorrelation_5'] = feature_calculators.autocorrelation(x, 5)\n",
    "        feature_dict['autocorrelation_10'] = feature_calculators.autocorrelation(x, 10)\n",
    "        feature_dict['autocorrelation_50'] = feature_calculators.autocorrelation(x, 50)\n",
    "        feature_dict['autocorrelation_100'] = feature_calculators.autocorrelation(x, 100)\n",
    "        feature_dict['autocorrelation_1000'] = feature_calculators.autocorrelation(x, 1000)\n",
    "        feature_dict['c3_5'] = feature_calculators.c3(x, 5)\n",
    "        feature_dict['c3_10'] = feature_calculators.c3(x, 10)\n",
    "        feature_dict['c3_100'] = feature_calculators.c3(x, 100)\n",
    "        feature_dict['fft_1_real'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 1, 'attr': 'real'}]))[0][1]\n",
    "        feature_dict['fft_1_imag'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 1, 'attr': 'imag'}]))[0][1]\n",
    "        feature_dict['fft_1_ang'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 1, 'attr': 'angle'}]))[0][1]\n",
    "        feature_dict['fft_2_real'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 2, 'attr': 'real'}]))[0][1]\n",
    "        feature_dict['fft_2_imag'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 2, 'attr': 'imag'}]))[0][1]\n",
    "        feature_dict['fft_2_ang'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 2, 'attr': 'angle'}]))[0][1]\n",
    "        feature_dict['fft_3_real'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 3, 'attr': 'real'}]))[0][1]\n",
    "        feature_dict['fft_3_imag'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 3, 'attr': 'imag'}]))[0][1]\n",
    "        feature_dict['fft_3_ang'] = list(feature_calculators.fft_coefficient(x, [{'coeff': 3, 'attr': 'angle'}]))[0][1]\n",
    "        feature_dict['long_strk_above_mean'] = feature_calculators.longest_strike_above_mean(x)\n",
    "        feature_dict['long_strk_below_mean'] = feature_calculators.longest_strike_below_mean(x)\n",
    "        feature_dict['cid_ce_0'] = feature_calculators.cid_ce(x, 0)\n",
    "        feature_dict['cid_ce_1'] = feature_calculators.cid_ce(x, 1)\n",
    "        feature_dict['binned_entropy_5'] = feature_calculators.binned_entropy(x, 5)\n",
    "        feature_dict['binned_entropy_10'] = feature_calculators.binned_entropy(x, 10)\n",
    "        feature_dict['binned_entropy_20'] = feature_calculators.binned_entropy(x, 20)\n",
    "        feature_dict['binned_entropy_50'] = feature_calculators.binned_entropy(x, 50)\n",
    "        feature_dict['binned_entropy_80'] = feature_calculators.binned_entropy(x, 80)\n",
    "        feature_dict['binned_entropy_100'] = feature_calculators.binned_entropy(x, 100)\n",
    "\n",
    "        feature_dict['num_crossing_0'] = feature_calculators.number_crossing_m(x, 0)\n",
    "        feature_dict['num_peaks_10'] = feature_calculators.number_peaks(x, 10)\n",
    "        feature_dict['num_peaks_50'] = feature_calculators.number_peaks(x, 50)\n",
    "        feature_dict['num_peaks_100'] = feature_calculators.number_peaks(x, 100)\n",
    "        feature_dict['num_peaks_500'] = feature_calculators.number_peaks(x, 500)\n",
    "\n",
    "        feature_dict['spkt_welch_density_1'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': 1}]))[0][1]\n",
    "        feature_dict['spkt_welch_density_10'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': 10}]))[0][1]\n",
    "        feature_dict['spkt_welch_density_50'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': 50}]))[0][1]\n",
    "        feature_dict['spkt_welch_density_100'] = list(feature_calculators.spkt_welch_density(x, [{'coeff': 100}]))[0][1]\n",
    "\n",
    "        feature_dict['time_rev_asym_stat_1'] = feature_calculators.time_reversal_asymmetry_statistic(x, 1)\n",
    "        feature_dict['time_rev_asym_stat_10'] = feature_calculators.time_reversal_asymmetry_statistic(x, 10)\n",
    "        feature_dict['time_rev_asym_stat_100'] = feature_calculators.time_reversal_asymmetry_statistic(x, 100)        \n",
    "\n",
    "        return feature_dict\n",
    "\n",
    "    def generate(self):\n",
    "        feature_list = []\n",
    "        res = Parallel(n_jobs=self.n_jobs,\n",
    "                       backend='threading')(delayed(self.features)(x, y, s)\n",
    "                                            for s, x, y in tqdm(self.read_chunks(), total=self.total_data))\n",
    "        for r in res:\n",
    "            feature_list.append(r)\n",
    "        return pd.DataFrame(feature_list)\n",
    "\n",
    "\n",
    "training_fg = FeatureGenerator(dtype='train', n_jobs=10, chunk_size=150000)\n",
    "training_data = training_fg.generate()\n",
    "\n",
    "test_fg = FeatureGenerator(dtype='test', n_jobs=10, chunk_size=150000)\n",
    "test_data = test_fg.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_data.drop(['target', 'seg_id'], axis=1)\n",
    "X_test = test_data.drop(['target', 'seg_id'], axis=1)\n",
    "test_segs = test_data.seg_id\n",
    "y = training_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_preds = np.zeros((len(X), 1))\n",
    "test_preds = np.zeros((len(X_test), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"max_depth\": 4,\n",
    "    \"n_estimators\": 10000,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"subsample\": 1.0,\n",
    "    \"nthread\": 12,\n",
    "    \"random_state\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_, (trn_, val_) in enumerate(folds.split(X)):\n",
    "    print(\"Current Fold: {}\".format(fold_))\n",
    "    trn_x, trn_y = X.iloc[trn_], y.iloc[trn_]\n",
    "    val_x, val_y = X.iloc[val_], y.iloc[val_]\n",
    "\n",
    "    clf = xgb.XGBRegressor(**params)\n",
    "    clf.fit(\n",
    "        trn_x, trn_y,\n",
    "        eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "        eval_metric='mae',\n",
    "        verbose=150,\n",
    "        early_stopping_rounds=100\n",
    "    )\n",
    "    val_pred = clf.predict(val_x, ntree_limit=clf.best_ntree_limit)\n",
    "    test_fold_pred = clf.predict(X_test, ntree_limit=clf.best_ntree_limit)\n",
    "    print(\"MAE = {}\".format(metrics.mean_absolute_error(val_y, val_pred)))\n",
    "    oof_preds[val_, :] = val_pred.reshape((-1, 1))\n",
    "    test_preds += test_fold_pred.reshape((-1, 1))\n",
    "test_preds /= 5\n",
    "\n",
    "oof_score = metrics.mean_absolute_error(y, oof_preds)\n",
    "print(\"Mean MAE = {}\".format(oof_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(columns=['seg_id', 'time_to_failure'])\n",
    "submission.seg_id = test_segs\n",
    "submission.time_to_failure = test_preds\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
